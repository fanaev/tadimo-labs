{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fanaev/tadimo-labs/blob/main/blank__05_NLP_1_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "XtFQP3RNll3c"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "443CZCLp0_sE",
        "outputId": "8acfc7dd-a39d-4d6e-db10-19c2415a4484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1Y_-Dvw1akc"
      },
      "source": [
        "## Train func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jCKWu_c91ZwP"
      },
      "outputs": [],
      "source": [
        "def train_model(_model, dataloaders, criterion, \n",
        "                optimizer, num_epochs = 10, acc_stop_value = 0.9,\n",
        "                num_classes = 2, lr = 1e-3):\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  _model = _model.to(device)\n",
        "\n",
        "  train_losses, test_losses = [], []\n",
        "\n",
        "  _trainloader = dataloaders[0]\n",
        "  _valloader = dataloaders[1]\n",
        "  for e in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    print(f'Epo: {e}')\n",
        "    print('Train step')\n",
        "    for X, y in tqdm(trainloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      output = model(X)\n",
        "      loss = criterion(output, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      running_loss += loss.item()\n",
        "    else:\n",
        "      test_loss = 0\n",
        "      accuracy = 0\n",
        "      \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        print('Test step')\n",
        "        for X, y in tqdm(testloader):\n",
        "          X, y = X.to(device), y.to(device)\n",
        "          log_ps = model(X)\n",
        "          test_loss += criterion(log_ps, y)\n",
        "          \n",
        "          ps = torch.exp(log_ps)\n",
        "          top_p, top_class = ps.topk(1, dim = 1)\n",
        "          equals = top_class == y.view(*top_class.shape)\n",
        "          accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "      \n",
        "        train_losses.append(running_loss/len(trainloader))\n",
        "        test_losses.append(test_loss/len(testloader))\n",
        "        print('---------------------')\n",
        "        print(\"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n",
        "            \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n",
        "            \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqDHq_AEjRZ1"
      },
      "source": [
        "## 1. Представление и предобработка текстовых данных "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaki7efDpmXo"
      },
      "source": [
        "1.1 Операции по предобработке:\n",
        "* токенизация\n",
        "* стемминг / лемматизация\n",
        "* удаление стоп-слов\n",
        "* удаление пунктуации\n",
        "* приведение к нижнему регистру\n",
        "* любые другие операции над текстом"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nHRy4jpYphEr"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lMMzGhq0ikz1"
      },
      "outputs": [],
      "source": [
        "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUhfertRtXE5"
      },
      "source": [
        "Реализовать функцию `preprocess_text(text: str)`, которая:\n",
        "* приводит строку к нижнему регистру\n",
        "* заменяет все символы, кроме a-z, A-Z и знаков .,!? на пробел\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "6G7z23zoOpla"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text: str):\n",
        "  text = text.lower()\n",
        "  pattern = r'[^a-zA-Z0-9.,!?\\s]'\n",
        "\n",
        "  text = re.sub(string = text, pattern = pattern, repl = '')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "pHLg9GJdeJJj",
        "outputId": "5bb69f9a-3427-4d30-beed-f31eede1c175"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'select your preferences and run the install command. stable represents the most currently tested and supported version of pytorch. note that libtorch is only available for c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.sub(pattern =  r'\\s+', string = 'hi   susa   ', repl = '')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tFHsN_2ncxWc",
        "outputId": "f1c8e2e9-9403-48a7-d485-5b397c57339d"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hisusa'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2Dt1ssIqckC"
      },
      "source": [
        "1.2 Представление текстовых данных при помощи бинарного кодирования\n",
        "\n",
        "\n",
        "Представить первое предложение из `text` в виде тензора `sentence_t`: `sentence_t[i] == 1`, если __слово__ с индексом `i` присуствует в предложении."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOafeGDcHKjG"
      },
      "outputs": [],
      "source": [
        "# Это задание реализовано в \"Классификация обзоров ресторанов\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Nz_zcgw3N4"
      },
      "source": [
        "## 2. Классификация фамилий по национальности\n",
        "\n",
        "Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1\n",
        "\n",
        "2.1 Считать файл `surnames/surnames.csv`. \n",
        "\n",
        "2.2 Закодировать национальности числами, начиная с 0.\n",
        "\n",
        "2.3 Разбить датасет на обучающую и тестовую выборку\n",
        "\n",
        "2.4 Реализовать класс `Vocab` (токен = __символ__)\n",
        "\n",
        "2.5 Реализовать класс `SurnamesDataset`\n",
        "\n",
        "2.6. Обучить классификатор.\n",
        "\n",
        "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCYSJuRb4ja5"
      },
      "source": [
        "### Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce6rdXdTIljm",
        "outputId": "5e9929a0-ccac-4417-be66-3f63a9bb88ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 623.50it/s]\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "zf = zipfile.ZipFile('/content/surnames.zip')\n",
        "for file in tqdm(zf.infolist()):\n",
        "    zf.extract(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "PliIV9ckIyfl"
      },
      "outputs": [],
      "source": [
        "df_surnames = pd.read_csv('/content/surnames/surnames.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "kUkSZkDqxNYS"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "  def __init__(self, data):\n",
        "    data = data.surname.unique().astype('str')\n",
        "    self.idx_to_token = {}\n",
        "    self.token_to_idx = {}\n",
        "    self.gennerate_vocab(data)\n",
        "    self.vocab_len = len(self.token_to_idx)\n",
        "  def add_token(self, token):\n",
        "    if token not in self.token_to_idx:\n",
        "      self.token_to_idx[token] = len(self.token_to_idx)\n",
        "      self.idx_to_token[len(self.idx_to_token)] = token\n",
        "\n",
        "  def gennerate_vocab(self, data):\n",
        "    for surname in data:\n",
        "      for char in preprocess_text(surname):\n",
        "        self.add_token(char)\n",
        "\n",
        "nationalities = df_surnames.nationality.unique()\n",
        "vocab_nat = {nationalities[i]: i for i in range(len(nationalities))}\n",
        "reverse_nat =  {i: nationalities[i] for i in range(len(nationalities))}\n",
        "vocab = Vocab(df_surnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxMeUhKOlSQW",
        "outputId": "209e7be7-77c2-4f92-ba3f-c529af895fd5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ],
      "source": [
        "vocab.vocab_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "WCaRK1QHxe0A"
      },
      "outputs": [],
      "source": [
        "class SurnamesDataset(Dataset):\n",
        "  def __init__(self, X, y, vocab: Vocab):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.vocab = vocab\n",
        "\n",
        "  def vectorize(self, surname):\n",
        "    '''Генерирует представление фамилии surname в при помощи бинарного кодирования (см. 1.2)'''\n",
        "    repr = torch.zeros(self.vocab.vocab_len)\n",
        "    for char in preprocess_text(surname):\n",
        "      idx = self.vocab.token_to_idx[char]\n",
        "      repr[idx] = 1\n",
        "    return repr\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.vectorize(self.X[idx]), vocab_nat[self.y[idx]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "dppVvg2OUVwU"
      },
      "outputs": [],
      "source": [
        "dataset = SurnamesDataset(X = df_surnames.surname, y = df_surnames.nationality, vocab = vocab)\n",
        "\n",
        "train_size = int(dataset.__len__()*0.8)\n",
        "test_size = dataset.__len__() - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, shuffle = True, pin_memory=True)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size = 1, pin_memory = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_7DKknk4n0y"
      },
      "source": [
        "### Обучение MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxqLSl_3TlUg",
        "outputId": "2bd2b2de-dd3b-4266-b8d4-55670262e23f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ],
      "source": [
        "model = nn.Sequential(\n",
        "    nn.LazyLinear(512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.LazyLinear(1024),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.LazyLinear(len(vocab_nat)),\n",
        ")\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8bTTMI-VGoN",
        "outputId": "df650ef6-44c1-46a4-cb3f-2dc8674979ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epo: 0\n",
            "Train step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 275/275 [00:00<00:00, 293.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2196/2196 [00:01<00:00, 1576.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "Training loss: 1.662.. Test loss: 1.457.. Test Accuracy: 0.566\n",
            "Epo: 1\n",
            "Train step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 275/275 [00:00<00:00, 290.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2196/2196 [00:01<00:00, 1575.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "Training loss: 1.367.. Test loss: 1.329.. Test Accuracy: 0.612\n",
            "Epo: 2\n",
            "Train step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 275/275 [00:00<00:00, 298.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2196/2196 [00:01<00:00, 1574.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "Training loss: 1.241.. Test loss: 1.300.. Test Accuracy: 0.607\n",
            "Epo: 3\n",
            "Train step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 275/275 [00:00<00:00, 292.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2196/2196 [00:01<00:00, 1597.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "Training loss: 1.177.. Test loss: 1.266.. Test Accuracy: 0.616\n",
            "Epo: 4\n",
            "Train step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 275/275 [00:00<00:00, 302.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2196/2196 [00:01<00:00, 1607.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "Training loss: 1.130.. Test loss: 1.258.. Test Accuracy: 0.627\n",
            "Epo: 5\n",
            "Train step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 275/275 [00:00<00:00, 296.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2196/2196 [00:01<00:00, 1590.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "Training loss: 1.077.. Test loss: 1.238.. Test Accuracy: 0.622\n",
            "Epo: 6\n",
            "Train step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 275/275 [00:00<00:00, 296.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2196/2196 [00:01<00:00, 1598.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "Training loss: 1.031.. Test loss: 1.236.. Test Accuracy: 0.629\n",
            "Epo: 7\n",
            "Train step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 275/275 [00:00<00:00, 302.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2196/2196 [00:01<00:00, 1622.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "Training loss: 0.990.. Test loss: 1.217.. Test Accuracy: 0.630\n",
            "Epo: 8\n",
            "Train step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 275/275 [00:00<00:00, 299.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2196/2196 [00:01<00:00, 1516.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "Training loss: 0.957.. Test loss: 1.232.. Test Accuracy: 0.633\n",
            "Epo: 9\n",
            "Train step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 275/275 [00:00<00:00, 290.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2196/2196 [00:01<00:00, 1608.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "Training loss: 0.911.. Test loss: 1.235.. Test Accuracy: 0.637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_model(_model = model,\n",
        "              dataloaders = [trainloader, trainloader],\n",
        "              criterion = criterion,\n",
        "              optimizer = optimizer\n",
        "              )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eval"
      ],
      "metadata": {
        "id": "zQscR_Bjkxg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "surnames = ['Topson', 'Xio', 'Ivanov']\n",
        "with torch.no_grad():\n",
        "  for surname in surnames:\n",
        "    log_ps = model(dataset.vectorize(surname).to('cuda:0'))\n",
        "    ps = torch.exp(log_ps)\n",
        "    nationality = reverse_nat[ps.topk(1)[1].item()]\n",
        "    print('Предсказанная национальдность для фамилии ' + surname + ':', \n",
        "          nationality\n",
        "          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xdvb-bQjkafY",
        "outputId": "d385c214-bf56-4ed4-d012-3213055a4a8e"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказанная национальдность для фамилии Topson: English\n",
            "Предсказанная национальдность для фамилии Xio: Chinese\n",
            "Предсказанная национальдность для фамилии Ivanov: Russian\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLmDB3fJtVox"
      },
      "source": [
        "## 3. Классификация обзоров ресторанов\n",
        "\n",
        "Датасет: https://disk.yandex.ru/d/nY1o70JtAuYa8g\n",
        "\n",
        "3.1 Считать файл `yelp/raw_train.csv`. Оставить от исходного датасета 10% строчек.\n",
        "\n",
        "3.2 Воспользоваться функцией `preprocess_text` из 1.1 для обработки текста отзыва. Закодировать рейтинг числами, начиная с 0.\n",
        "\n",
        "3.3 Разбить датасет на обучающую и тестовую выборку\n",
        "\n",
        "3.4 Реализовать класс `Vocab` (токен = слово)\n",
        "\n",
        "3.5 Реализовать класс `ReviewDataset`\n",
        "\n",
        "3.6 Обучить классификатор\n",
        "\n",
        "3.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подготовка данных"
      ],
      "metadata": {
        "id": "beYM67rxX-Xd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "_lCTSKZgu68K"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "    self.idx_to_token = {}\n",
        "    self.token_to_idx = {}\n",
        "    self.gennerate_vocab()\n",
        "    self.vocab_len = len(self.token_to_idx)\n",
        "  def add_token(self, token):\n",
        "    if token not in self.token_to_idx:\n",
        "      self.token_to_idx[token] = len(self.token_to_idx)\n",
        "      self.idx_to_token[len(self.idx_to_token)] = token\n",
        "\n",
        "  def gennerate_vocab(self):\n",
        "    for row in self.data:\n",
        "      for word in row.split(' '):\n",
        "        self.add_token(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "WXLmCDvcvRmb"
      },
      "outputs": [],
      "source": [
        "class ReviewDataset(Dataset):\n",
        "  def __init__(self, X, y, vocab: Vocab):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.vocab = vocab\n",
        "\n",
        "  def vectorize(self, review):\n",
        "    '''Генерирует представление фамилии surname в при помощи бинарного кодирования (см. 1.2)'''\n",
        "    repr = torch.zeros(self.vocab.vocab_len)\n",
        "    for word in review.split(' '):\n",
        "      idx = self.vocab.token_to_idx[word]\n",
        "      repr[idx] = 1\n",
        "    return repr\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.vectorize(self.X[idx]), self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0myP2D9E7kxE",
        "outputId": "dd3f461d-5754-4178-a9d6-335356771d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3326: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "Skipping line 23513: Expected 2 fields in line 23513, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n",
            "Skipping line 126780: Expected 2 fields in line 126780, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.\n"
          ]
        }
      ],
      "source": [
        "traindata = pd.read_csv(\n",
        "    '/content/raw_train.csv', \n",
        "    sep = '\",\"', \n",
        "    error_bad_lines=False, \n",
        "    header = None\n",
        "    ).sample(frac = 0.1)\n",
        "\n",
        "X = traindata[[1]].apply(\n",
        "    lambda x: preprocess_text(x.iloc[0]), \n",
        "    axis = 1\n",
        "    ).values\n",
        "labels = traindata[[0]].apply(\n",
        "    lambda x: int(x.iloc[0].replace('\"', '')), \n",
        "    axis = 1\n",
        "    ).values - 1 # [1, 2] -> [0, 1]\n",
        "word_vocab = Vocab(X)\n",
        "\n",
        "del traindata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ReviewDataset(X = X, y = labels, vocab = word_vocab)\n",
        "\n",
        "train_size = int(dataset.__len__()*0.8)\n",
        "test_size = dataset.__len__() - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, shuffle = True, pin_memory=True)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size = 1, pin_memory = True)"
      ],
      "metadata": {
        "id": "hyq6Lo7daBz_"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.__getitem__(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BqcgLE8aTDP",
        "outputId": "890116ca-8c12-444a-cce3-f9e280d187fa"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1., 1., 0.,  ..., 0., 0., 0.]), 1)"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение MLP"
      ],
      "metadata": {
        "id": "xjzkLzlYgo-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.LazyLinear(512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.LazyLinear(1024),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.LazyLinear(2),\n",
        ")\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 1"
      ],
      "metadata": {
        "id": "YoV7DcRKcRj5"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(_model = model,\n",
        "              dataloaders = [trainloader, trainloader],\n",
        "              criterion = criterion,\n",
        "              optimizer = optimizer\n",
        "              )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZG2UyzsgShp",
        "outputId": "518ea6b2-8fe0-4982-8d13-364d00ef717d"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epo: 0\n",
            "Train step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1400/1400 [02:04<00:00, 11.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11200/11200 [00:36<00:00, 311.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "Training loss: 0.270.. Test loss: 0.214.. Test Accuracy: 0.913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eval"
      ],
      "metadata": {
        "id": "G04fARL9jmjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos = 'like very much'"
      ],
      "metadata": {
        "id": "WfwYb7J-h-8Y"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neg = 'so bad'"
      ],
      "metadata": {
        "id": "8bAHYQ41if-V"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  print(\"Предсказание для позивного примера:\",\n",
        "      torch.exp(model(dataset.vectorize(pos).to('cuda:0'))).topk(1)[1].item()\n",
        "      )\n",
        "  print(\"Предсказание для негативного примера:\",\n",
        "      torch.exp(model(dataset.vectorize(neg).to('cuda:0'))).topk(1)[1].item()\n",
        "      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cYNg7cJiz9r",
        "outputId": "6f781bad-2d49-47cc-c8da-39bc9d5924d5"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказание для позивного примера: 1\n",
            "Предсказание для негативного примера: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rb3Bl87Hj4wt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}